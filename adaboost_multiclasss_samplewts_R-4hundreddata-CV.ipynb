{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fh=read.csv(\"four-hundred.csv\",sep=\",\",header=F)\n",
    "#fh=fh[,-c(2,3,14)]\n",
    "#length(which(is.na(fh[,1])==T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 5 #Folds\n",
    "\n",
    "# sample from 1 to k, nrow times (the number of observations in the data)\n",
    "fh$id <- sample(1:k, nrow(fh), replace = TRUE)\n",
    "list <- 1:k\n",
    "cv<-function(f,data){\n",
    "    X=data[,c(-1)]\n",
    "y=data[,c(1)]\n",
    "for(i in 1:ncol(X)){\n",
    "  X[is.na(X[,i]), i] <- mean(X[,i], na.rm = TRUE)\n",
    "}\n",
    "\n",
    "#y[is.na(y)]<-10\n",
    "\n",
    "    d=cbind(X,y)\n",
    "   # print((d$id))\n",
    "  # remove rows with id i from dataframe to create training set\n",
    "  # select rows with id i to create test set\n",
    "  trainingset <- subset(d, id %in% list[-f])\n",
    "  testset <- subset(d, id %in% c(f))\n",
    "   #print(which(d$id==f))\n",
    "  #  testset <- d[which(d$id == i),]#subset(d, id %in% list[i])\n",
    "X.train<-trainingset[,-c(ncol(trainingset),ncol(trainingset)-1)]\n",
    "X.test<-testset[,-c(ncol(trainingset),ncol(trainingset)-1)]\n",
    "y.train<-trainingset[,ncol(trainingset)]\n",
    "y.test<-testset[,ncol(trainingset)]\n",
    "\n",
    "return(list(X.train=X.train,y.train=y.train,X.test=X.test,y.test=y.test) )   \n",
    "}\n",
    "\n",
    "#cv(2,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(\"wSVM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(gmum.r)\n",
    "library(SparseM)\n",
    "library(Matrix)\n",
    "library(ggplot2)\n",
    "        \n",
    "#svm.wt <- SVM(X.train,y.train, core=\"svmlight\", kernel=\"rbf\", C=1.0, \n",
    "                      #  gamma=0.5, example.weights=dm)\n",
    "\n",
    "#summary(svm.wt)\n",
    "#predicted <- predict(svm.wt,X.train)\n",
    "#(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in SVM.default(X.train, y.train, core = \"svmlight\", kernel = \"rbf\", :\n",
      "“It is recommended to pass y as factor”"
     ]
    }
   ],
   "source": [
    "acc=c()\n",
    "for (K in 1:k ){\n",
    "    print(K)\n",
    "    \n",
    "    #split train test\n",
    "X.train<-cv(K,fh)$X.train\n",
    "y.train<-cv(K,fh)$y.train\n",
    "X.test<-cv(K,fh)$X.test\n",
    "y.test<-cv(K,fh)$y.test\n",
    "    \n",
    "\n",
    "    ###set max iterations T\n",
    "T=2\n",
    "#initialize sample weights\n",
    "dm=replicate(nrow(X.train),1/nrow(X.train))\n",
    "#length(dm)\n",
    "    \n",
    "###aplha parameter from adaboost algorith \n",
    "alpha=c()\n",
    "\n",
    "###list to store predictions\n",
    "pred=list()\n",
    "\n",
    "###list to store test vector predictions\n",
    "testpred=list()\n",
    "\n",
    "###define number of classes\n",
    "classes=11\n",
    "#dm\n",
    "\n",
    "###run adabosst multiclass with cost vector to each class\n",
    "for (t in 1:T){\n",
    "svm.wt <- SVM(X.train,y.train, core=\"svmlight\", kernel=\"rbf\", C=1.0, \n",
    "                        gamma=0.5, example.weights=dm)\n",
    "\n",
    "#summary(svm.wt)\n",
    "#predict test values\n",
    "predicted_test <- predict(svm.wt,X.test)\n",
    "#predict train values\n",
    "predicted_train<- predict(svm.wt,X.train)    \n",
    "\n",
    "    \n",
    "#store predicted test values\n",
    "testpred[[t]]<-(predicted_test)\n",
    "    #print(testpred)\n",
    "    \n",
    "#store predicted train values\n",
    "pred[[t]]<-(predicted_train)\n",
    "    \n",
    "#initially for each iteration error==0\n",
    "err=0\n",
    "    \n",
    "    #for each example in train predictions check misclassification\n",
    "    for (i in (1:nrow(X.train))){\n",
    "        if (pred[[t]][i] != y.train[i]){\n",
    "            y=y.train[i]\n",
    "            err=err + dm[i] #* cost[y-1]\n",
    "            }\n",
    "        }\n",
    "    #calculate alpha wth err value and classes\n",
    "    alpha[t]<-(log((1-err)/err) + log(classes-1))\n",
    "    \n",
    "    #calculate Z value for normaliztion\n",
    "    Z=c()\n",
    "    #print(err)\n",
    "    for (i in (1:nrow(X.train))){\n",
    "        #print(dm)\n",
    "        #print(as.numeric( predicted_train[i]))\n",
    "        Z[i]<-(dm[i] * exp((-(alpha[t]))*(as.numeric(y.train[i])) *as.numeric( predicted_train[i])))\n",
    "        #print(Z)\n",
    "    }\n",
    "    #update the weights\n",
    "    \n",
    "    for (i in (1:nrow(X.train))){\n",
    "        dm[i]=Z[i] / sum(Z)\n",
    "        #print(dm)\n",
    "        }\n",
    "\n",
    "        \n",
    "}   \n",
    "\n",
    "#initialize the adaboost ensemble otput for each class\n",
    "ada_class=c()\n",
    "ada_class_pred=list()\n",
    "#run for loop for number of classes\n",
    "for (c in (1:classes)){\n",
    "    \n",
    "    #initialize predictions for each test vector\n",
    "    \n",
    "    \n",
    "    #for each test vector \n",
    "    #for (i in 1:nrow(testpred[t]) ){\n",
    "        \n",
    "        #initialize the adaboost output as zero for each class\n",
    "        ada_predicts=replicate(length(testpred[[1]]),0)\n",
    "        #calculate the adaboost predicted value across t generations.this is a scalar\n",
    "        for (t in 1:T){\n",
    "            vec=testpred[[t]]\n",
    "            al=alpha[t]\n",
    "           # print(vec)\n",
    "            for (i in 1:length(vec)){\n",
    "            if (vec[i]==c){\n",
    "                ada_predicts[i]= ada_predicts[i]+(al *1)\n",
    "                }\n",
    "            else{\n",
    "                ada_predicts[i]=ada_predicts[i]+(al *0)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    ada_class_pred[[c]]<-(ada_predicts)\n",
    "        \n",
    "    #}\n",
    "    #predictions of each test vector across classes\n",
    "    #ada_class.append(ada_class_pred)\n",
    "}\n",
    "#ada_class_pred\n",
    "\n",
    "x<-ada_class_pred\n",
    "cl=c()\n",
    "for (i in 1:length(testpred[[1]]))\n",
    "{\n",
    "m = as.numeric(do.call( rbind, x)[,i])\n",
    "    #print(m)\n",
    "cl[i]<-which(m==max(m))[1]\n",
    "}\n",
    "#\n",
    "\n",
    "acc[K] = length(which(y.test == cl)) / length(cl)\n",
    "    print(acc[K])\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg.acc=sum(acc)/k\n",
    "avg.acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in SVM.default(X.train, y.train, kernel = \"rbf\", C = 1, gamma = 0.5, :\n",
      "“It is recommended to pass y as factor”"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimization finished, #iter = 6222\n",
      "optimization finished, #iter = 8694\n",
      "optimization finished, #iter = 3810\n",
      "optimization finished, #iter = 4930\n",
      "optimization finished, #iter = 5912\n",
      "optimization finished, #iter = 6305\n",
      "optimization finished, #iter = 2956\n",
      "optimization finished, #iter = 142\n",
      "optimization finished, #iter = 1225\n",
      "optimization finished, #iter = 502\n",
      "optimization finished, #iter = 255\n"
     ]
    }
   ],
   "source": [
    "X.train<-cv(1,fh)$X.train\n",
    "y.train<-cv(1,fh)$y.train\n",
    "X.test<-cv(1,fh)$X.test\n",
    "y.test<-cv(1,fh)$y.test\n",
    "svm.wt <- SVM(X.train,y.train, kernel=\"rbf\", C=1.0, \n",
    "                        gamma=0.5, example.weights=dm)\n",
    "\n",
    "#summary(svm.wt)\n",
    "#predict test values\n",
    "predicted_test <- predict(svm.wt,X.test)\n",
    "#predict train values\n",
    "predicted_train<- predict(svm.wt,X.train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
